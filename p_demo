import os
import shutil
import subprocess
from pathlib import Path
from typing import Optional, List

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
import tqdm as tq

os.environ['KAGGLE_USERNAME'] = 'harshvardhangupta15'
os.environ['KAGGLE_KEY'] = '55e5d21ededb86fa4797d2d6de2a3fd4'

class Extract:
    def __init__(
        self,
        dataset: str = "sobhanmoosavi/us-accidents",
        extract_dir: str = r"D:\OneDrive - Coforge Limited\Desktop\harshfinalproject\us_accidents",
        delete_zip_after_unzip: bool = True,
        encoding: str = "utf-8",
        verbose: bool = True,
    ):
        self.dataset = dataset
        self.extract_dir = Path(extract_dir)
        self.extract_dir.mkdir(parents=True, exist_ok=True)
        self.delete_zip_after_unzip = delete_zip_after_unzip
        self.encoding = encoding
        self.verbose = verbose

        # 1) Check Kaggle CLI
        if shutil.which("kaggle") is None:
            raise EnvironmentError(
                "Kaggle CLI not found. Install with 'pip install kaggle' and ensure 'kaggle' is on PATH.\n"
               
            )

        # 2) If no CSV yet, download and unzip
        if not self._has_csv_in_dir():
            if self.verbose:
                print(f"Downloading {self.dataset} to: {self.extract_dir}")
            cmd = [
                "kaggle", "datasets", "download",
                "-d", self.dataset,
                "-p", str(self.extract_dir),
                "--unzip"
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode != 0:
                raise RuntimeError(
                    "Kaggle download failed.\n"
                    f"STDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}\n\n"
                    "Check Kaggle credentials and dataset slug."
                )

            if self.verbose:
                print("Download and unzip complete.")

            if self.delete_zip_after_unzip:
                for z in self.extract_dir.glob("*.zip"):
                    try:
                        z.unlink()
                    except Exception:
                        pass

        # 3) Locate the US Accidents CSV
        csv_path = self._find_accidents_csv()
        if not csv_path:
            raise FileNotFoundError(
                f"No US_Accidents*.csv found in {self.extract_dir}. "
                "Please verify the dataset contents."
            )

        self.csv_path = str(csv_path)
        if self.verbose:
            print(f"Using CSV: {self.csv_path}")

        # 4) Load the CSV with a robust fallback for encoding
        try:
            self.df = pd.read_csv(self.csv_path, encoding=self.encoding, low_memory=False)
        except UnicodeDecodeError:
            # Try a BOM-aware encoding if the first attempt fails
            self.df = pd.read_csv(self.csv_path, encoding="utf-8-sig", low_memory=False)

    # ----------------- NEW: helper methods that were missing -----------------

    def _has_csv_in_dir(self) -> bool:
        """
        Returns True if any CSV that looks like the US Accidents dataset exists in extract_dir.
        """
        patterns: List[str] = ["US_Accidents*.csv", "*.csv"]
        for pat in patterns:
            if any(self.extract_dir.glob(pat)):
                return True
        return False

    def _find_accidents_csv(self) -> Optional[Path]:
        """
        Returns a Path to the first CSV found (prefer US_Accidents*.csv).
        """
        preferred = sorted(self.extract_dir.glob("US_Accidents*.csv"))
        if preferred:
            # If multiple versions exist, pick the newest by name (rough heuristic)
            return preferred[-1]
        # Fallback to any CSV
        any_csv = sorted(self.extract_dir.glob("*.csv"))
        return any_csv[-1] if any_csv else None


if __name__ == "__main__":
    extract = Extract(
        dataset="sobhanmoosavi/us-accidents",
        extract_dir=r"D:\OneDrive - Coforge Limited\Desktop\harshfinalproject\us_accidents",
        delete_zip_after_unzip=True,
        encoding="utf-8",
        verbose=True,
    )

    # Example: show the first 5 rows to confirm data loaded
    print(extract.df.head())















--------------------------------------------------------------------------------------------------------------------------------------

import os
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
 
# Set Kaggle API credentials (make sure kaggle.json is in ~/.kaggle or set env vars)
# os.environ['KAGGLE_USERNAME'] = 'your_username'
# os.environ['KAGGLE_KEY'] = 'your_key'
 
# Download the dataset using Kaggle API
os.system('kaggle datasets download -d gauravpathak1789/yellow-tripdata-2020-01 --unzip -p ./data')
 
# Find the CSV file (adjust filename if needed)
csv_path = './data/yellow_tripdata_2020-01.csv'
 
# Load data
df = pd.read_csv(csv_path)
 
print(f"Total rows fetched: {len(df)}")
 
# Example transformation: drop duplicates by 'VendorID'
df = df.drop_duplicates(subset="VendorID")
 
# Example transformation: add log of trip_distance
df["trip_distance_log"] = df["trip_distance"].apply(lambda x: np.log(x) if x > 0 else None)
 
# Example normalization
if "total_amount" in df.columns:
    df["total_amount_normalized"] = (df["total_amount"] - df["total_amount"].mean()) / df["total_amount"].std()
 
print("Transformations applied")
print(df.head(5))
 
# Save DataFrame to MySQL
mssql_engine = "mysql+mysqlconnector://root:cfg%401234@localhost/cryptodb"
df.to_sql("yellow_tripdata", mssql_engine, if_exists="replace", index=False)
print("Data inserted into MySQL")
 
# Query row count from MySQL
with create_engine(mssql_engine).connect() as conn:
    result = conn.execute(text("SELECT COUNT(*) FROM yellow_tripdata"))
    print("MySQL row count:", result.scalar())
 
 

